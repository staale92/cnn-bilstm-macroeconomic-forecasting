experiment:
  # Another variable (not investments) with CNNâ€“BiLSTM tuning enabled.
  name: smoke_tuned_univariate_multistep_consumption
  feature_mode: univariate
  horizon: 12
  lookback: 12
  stride: 1
  dataset_config: configs/dataset.yaml
  covariates_config: configs/covariates_table_a1.yaml
  baselines_config: configs/baselines.yaml
  results_dir: results
  targets:
    - consumption

training:
  # Keep reasonable; early stopping makes this faster in practice.
  epochs: 60
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.0
  loss: huber
  grad_clip_norm: 1.0
  max_norm: 3.0
  early_stopping: true
  patience: 12
  val_fraction: 0.1

model:
  cnn_bilstm:
    conv_filters: 64
    conv_kernel: 3
    pool_size: 2
    lstm_units: [200, 100, 50]
    fc_units: 25
    dropout_cnn: 0.1
    dropout_lstm: 0.5
    dropout_fc: 0.1
  bilstm:
    lstm_units: [200, 100, 50]
    fc_units: 25
    dropout_lstm: 0.5
    dropout_fc: 0.1

repeats:
  seeds: [0, 1]

tuning:
  enabled: true
  models: [cnn_bilstm]
  metric: mae
  # "Fuller" tuning than the earlier smoke runs; adjust upward for serious runs.
  n_trials: 25
  repeats: 1
  seed: 123
  max_epochs: 18
  patience: 4
  search_space:
    training:
      learning_rate: [0.0003, 0.001, 0.003]
      weight_decay: [0.0, 0.0001, 0.0005]
      loss: [huber, mse, mae]
      grad_clip_norm: [0.0, 1.0, 5.0]
    model:
      conv_filters: [32, 64, 128]
      conv_kernel: [3, 5, 7]
      pool_size: [1, 2]
      lstm_units:
        - [128, 64, 32]
        - [200, 100, 50]
      fc_units: [25, 50, 100]
      dropout_cnn: [0.0, 0.1, 0.2, 0.3]
      dropout_lstm: [0.2, 0.3, 0.5]
      dropout_fc: [0.0, 0.1, 0.2, 0.3]

plots:
  targets: [consumption]

