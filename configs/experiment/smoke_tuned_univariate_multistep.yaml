experiment:
  # End-to-end run that includes hyperparameter tuning (train/val only) to improve CNNâ€“BiLSTM.
  name: smoke_tuned_univariate_multistep
  feature_mode: univariate
  horizon: 12
  lookback: 12
  stride: 1
  dataset_config: configs/dataset.yaml
  covariates_config: configs/covariates_table_a1.yaml
  baselines_config: configs/baselines.yaml
  results_dir: results
  targets:
    - investments

training:
  epochs: 40
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.0
  loss: mse
  grad_clip_norm: 1.0
  max_norm: 3.0
  early_stopping: true
  patience: 10
  val_fraction: 0.1

model:
  cnn_bilstm:
    conv_filters: 64
    conv_kernel: 3
    pool_size: 2
    lstm_units: [200, 100, 50]
    fc_units: 25
    dropout_cnn: 0.1
    dropout_lstm: 0.5
    dropout_fc: 0.1
  bilstm:
    lstm_units: [200, 100, 50]
    fc_units: 25
    dropout_lstm: 0.5
    dropout_fc: 0.1

repeats:
  seeds: [0, 1]

tuning:
  enabled: true
  # Tune only the main model; baseline BiLSTM stays fixed for a fair comparison.
  models: [cnn_bilstm]
  metric: mae
  n_trials: 8
  seed: 123
  max_epochs: 12
  patience: 3
  search_space:
    training:
      learning_rate: [0.0003, 0.001, 0.003]
      weight_decay: [0.0, 0.0001]
      loss: [mse, huber]
      grad_clip_norm: [0.0, 1.0]
    model:
      conv_filters: [32, 64, 128]
      conv_kernel: [3, 5]
      fc_units: [25, 50]
      dropout_cnn: [0.0, 0.1, 0.2]
      dropout_lstm: [0.3, 0.5]
      dropout_fc: [0.0, 0.1, 0.2]

plots:
  targets: [investments]

